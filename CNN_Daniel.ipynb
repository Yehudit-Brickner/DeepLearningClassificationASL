{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into dataframe\n",
    "training_data_folder = 'asl_alphabet_train26'\n",
    "data = []\n",
    "\n",
    "for folder in sorted(os.listdir(training_data_folder)):\n",
    "    sub_folder = os.path.join(training_data_folder,folder)\n",
    "    files = [{'label':folder,'path':os.path.join(sub_folder, f)} for f in os.listdir(sub_folder) if os.path.isfile(os.path.join(sub_folder, f))]\n",
    "    data += files\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "map_characters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', \n",
    "                  10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', \n",
    "                  19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n",
    "\n",
    "# map_characters\n",
    "order_list=('A',  'B',  'C', 'D', 'E',  'F', 'G', 'H',  'I',  'J', \n",
    "                   'K',  'L',  'M',  'N',  'O',  'P',  'Q',  'R',  'S', \n",
    "                 'T',  'U',  'V',  'W',  'X',  'Y',  'Z')\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb=LabelBinarizer()\n",
    "hotEncodedLabels=lb.fit_transform(order_list)\n",
    "hotEncodedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir=\"asl_alphabet_train26\"\n",
    "\n",
    "map_characters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', \n",
    "                  10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', \n",
    "                  19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n",
    "\n",
    "\n",
    "labels_dict = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25}\n",
    "\n",
    "\n",
    "order_list=('A',  'B',  'C', 'D', 'E',  'F', 'G', 'H',  'I',  'J', \n",
    "                   'K',  'L',  'M',  'N',  'O',  'P',  'Q',  'R',  'S', \n",
    "                 'T',  'U',  'V',  'W',  'X',  'Y',  'Z')\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Loads data and preprocess. Returns train and test data along with labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    size = 64,64\n",
    "    num=0\n",
    "    print(\"LOADING DATA FROM : \",end = \"\")\n",
    "    for folder in os.listdir(train_dir):\n",
    "        print(folder, end = ' | ')\n",
    "        for image in os.listdir(train_dir + \"/\" + folder):\n",
    "            temp_img = cv2.imread(train_dir + '/' + folder + '/' + image)\n",
    "            temp_img = cv2.resize(temp_img, size)\n",
    "            # temp_img = temp_img.flatten()\n",
    "            images.append(temp_img)\n",
    "            labels.append(num)\n",
    "        num+=1\n",
    "    \n",
    "    images = np.array(images)\n",
    "    images = images.astype('float32')/255.0\n",
    "    \n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(images, labels, test_size = 0.3, random_state=42)\n",
    "    X_test, X_validation, Y_test, Y_validation = train_test_split(X_test, Y_test, test_size = 0.8,random_state=42)\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    print('Loaded', len(X_train),'images for training,','Train data shape =',X_train.shape)\n",
    "    print('Loaded', len(X_validation),'images for validation','validation data shape =',X_validation.shape)\n",
    "    print('Loaded', len(X_test),'images for testing','Test data shape =',X_test.shape)\n",
    "\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X_validation, Y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA FROM : A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | X | Y | Z | \n",
      "Loaded 54600 images for training, Train data shape = (54600, 64, 64, 3)\n",
      "Loaded 18720 images for validation validation data shape = (18720, 64, 64, 3)\n",
      "Loaded 4680 images for testing Test data shape = (4680, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test, X_validation, Y_validation= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 60, 60, 8)         608       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 30, 30, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 16)        3216      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 13, 13, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 9, 32)          12832     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               51300     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                2626      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,582\n",
      "Trainable params: 70,582\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model = keras.models.Sequential()\n",
    "   \n",
    "    model.add(layers.Conv2D(8,(5,5), strides=(1,1), padding=\"valid\", activation='relu', input_shape=(64,64,3)))\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "    model.add(layers.Conv2D(16, 5, activation='relu'))\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "    model.add(layers.Conv2D(32,5, activation='relu'))\n",
    "    model.add(layers.MaxPool2D((2,2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(26))\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    # loss and optimizer\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    optim = keras.optimizers.Adam(lr=0.001)\n",
    "    metrics = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    model.compile(optimizer=optim, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "5460/5460 - 131s - loss: 0.9013 - accuracy: 0.7184 - 131s/epoch - 24ms/step\n",
      "Epoch 2/30\n",
      "5460/5460 - 124s - loss: 0.2027 - accuracy: 0.9318 - 124s/epoch - 23ms/step\n",
      "Epoch 3/30\n",
      "5460/5460 - 124s - loss: 0.1263 - accuracy: 0.9587 - 124s/epoch - 23ms/step\n",
      "Epoch 4/30\n",
      "5460/5460 - 123s - loss: 0.0982 - accuracy: 0.9682 - 123s/epoch - 22ms/step\n",
      "Epoch 5/30\n",
      "5460/5460 - 121s - loss: 0.0834 - accuracy: 0.9743 - 121s/epoch - 22ms/step\n",
      "Epoch 6/30\n",
      "5460/5460 - 120s - loss: 0.0717 - accuracy: 0.9786 - 120s/epoch - 22ms/step\n",
      "Epoch 7/30\n",
      "5460/5460 - 112s - loss: 0.0724 - accuracy: 0.9790 - 112s/epoch - 20ms/step\n",
      "Epoch 8/30\n",
      "5460/5460 - 110s - loss: 0.0581 - accuracy: 0.9833 - 110s/epoch - 20ms/step\n",
      "Epoch 9/30\n",
      "5460/5460 - 109s - loss: 0.0594 - accuracy: 0.9832 - 109s/epoch - 20ms/step\n",
      "Epoch 10/30\n",
      "5460/5460 - 108s - loss: 0.0607 - accuracy: 0.9834 - 108s/epoch - 20ms/step\n",
      "Epoch 11/30\n",
      "5460/5460 - 108s - loss: 0.0542 - accuracy: 0.9851 - 108s/epoch - 20ms/step\n",
      "Epoch 12/30\n",
      "5460/5460 - 107s - loss: 0.0546 - accuracy: 0.9855 - 107s/epoch - 20ms/step\n",
      "Epoch 13/30\n",
      "5460/5460 - 107s - loss: 0.0510 - accuracy: 0.9860 - 107s/epoch - 20ms/step\n",
      "Epoch 14/30\n",
      "5460/5460 - 107s - loss: 0.0547 - accuracy: 0.9869 - 107s/epoch - 20ms/step\n",
      "Epoch 15/30\n",
      "5460/5460 - 104s - loss: 0.0496 - accuracy: 0.9878 - 104s/epoch - 19ms/step\n",
      "Epoch 16/30\n",
      "5460/5460 - 105s - loss: 0.0485 - accuracy: 0.9889 - 105s/epoch - 19ms/step\n",
      "Epoch 17/30\n",
      "5460/5460 - 104s - loss: 0.0532 - accuracy: 0.9884 - 104s/epoch - 19ms/step\n",
      "Epoch 18/30\n",
      "5460/5460 - 104s - loss: 0.0590 - accuracy: 0.9880 - 104s/epoch - 19ms/step\n",
      "Epoch 19/30\n",
      "5460/5460 - 104s - loss: 0.0534 - accuracy: 0.9894 - 104s/epoch - 19ms/step\n",
      "Epoch 20/30\n",
      "5460/5460 - 103s - loss: 0.0546 - accuracy: 0.9896 - 103s/epoch - 19ms/step\n",
      "Epoch 21/30\n",
      "5460/5460 - 103s - loss: 0.0499 - accuracy: 0.9903 - 103s/epoch - 19ms/step\n",
      "Epoch 22/30\n",
      "5460/5460 - 104s - loss: 0.0569 - accuracy: 0.9892 - 104s/epoch - 19ms/step\n",
      "Epoch 23/30\n",
      "5460/5460 - 104s - loss: 0.0550 - accuracy: 0.9898 - 104s/epoch - 19ms/step\n",
      "Epoch 24/30\n",
      "5460/5460 - 103s - loss: 0.0511 - accuracy: 0.9896 - 103s/epoch - 19ms/step\n",
      "Epoch 25/30\n",
      "5460/5460 - 104s - loss: 0.0484 - accuracy: 0.9906 - 104s/epoch - 19ms/step\n",
      "Epoch 26/30\n",
      "5460/5460 - 104s - loss: 0.0518 - accuracy: 0.9902 - 104s/epoch - 19ms/step\n",
      "Epoch 27/30\n",
      "5460/5460 - 103s - loss: 0.0463 - accuracy: 0.9916 - 103s/epoch - 19ms/step\n",
      "Epoch 28/30\n",
      "5460/5460 - 103s - loss: 0.0448 - accuracy: 0.9918 - 103s/epoch - 19ms/step\n",
      "Epoch 29/30\n",
      "5460/5460 - 103s - loss: 0.0472 - accuracy: 0.9919 - 103s/epoch - 19ms/step\n",
      "Epoch 30/30\n",
      "5460/5460 - 104s - loss: 0.0495 - accuracy: 0.9912 - 104s/epoch - 19ms/step\n",
      "eval\n",
      "1872/1872 - 10s - loss: 0.0707 - accuracy: 0.9885 - 10s/epoch - 5ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5460/5460 - 104s - loss: 0.0567 - accuracy: 0.9911 - 104s/epoch - 19ms/step\n",
      "Epoch 2/15\n",
      "5460/5460 - 104s - loss: 0.0491 - accuracy: 0.9920 - 104s/epoch - 19ms/step\n",
      "Epoch 3/15\n",
      "5460/5460 - 105s - loss: 0.0497 - accuracy: 0.9918 - 105s/epoch - 19ms/step\n",
      "Epoch 4/15\n",
      "5460/5460 - 103s - loss: 0.0576 - accuracy: 0.9918 - 103s/epoch - 19ms/step\n",
      "Epoch 5/15\n",
      "5460/5460 - 103s - loss: 0.0503 - accuracy: 0.9923 - 103s/epoch - 19ms/step\n",
      "Epoch 6/15\n",
      "5460/5460 - 103s - loss: 0.0515 - accuracy: 0.9926 - 103s/epoch - 19ms/step\n",
      "Epoch 7/15\n",
      "5460/5460 - 103s - loss: 0.0633 - accuracy: 0.9925 - 103s/epoch - 19ms/step\n",
      "Epoch 8/15\n",
      "5460/5460 - 103s - loss: 0.0515 - accuracy: 0.9932 - 103s/epoch - 19ms/step\n",
      "Epoch 9/15\n",
      "5460/5460 - 100s - loss: 0.0567 - accuracy: 0.9927 - 100s/epoch - 18ms/step\n",
      "Epoch 10/15\n",
      "5460/5460 - 98s - loss: 0.0570 - accuracy: 0.9929 - 98s/epoch - 18ms/step\n",
      "Epoch 11/15\n",
      "5460/5460 - 98s - loss: 0.0574 - accuracy: 0.9931 - 98s/epoch - 18ms/step\n",
      "Epoch 12/15\n",
      "5460/5460 - 98s - loss: 0.0561 - accuracy: 0.9938 - 98s/epoch - 18ms/step\n",
      "Epoch 13/15\n",
      "5460/5460 - 98s - loss: 0.0623 - accuracy: 0.9929 - 98s/epoch - 18ms/step\n",
      "Epoch 14/15\n",
      "5460/5460 - 98s - loss: 0.0579 - accuracy: 0.9938 - 98s/epoch - 18ms/step\n",
      "Epoch 15/15\n",
      "5460/5460 - 455s - loss: 0.0580 - accuracy: 0.9935 - 455s/epoch - 83ms/step\n",
      "eval\n",
      "1872/1872 - 10s - loss: 0.1106 - accuracy: 0.9884 - 10s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    data_x=np.array(X_train)\n",
    "    data_y=np.array(Y_train)\n",
    "\n",
    "    val_x=np.array(X_validation)\n",
    "    val_y=np.array(Y_validation)\n",
    "\n",
    "    # training\n",
    "    batch_size = 10\n",
    "    epochs = 30\n",
    "    model.fit(data_x, data_y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    print(\"eval\")\n",
    "    model.evaluate(val_x,  val_y, batch_size=batch_size, verbose=2)\n",
    "    \n",
    "    epochs = 15\n",
    "    optim = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.fit(data_x, data_y, epochs=epochs, batch_size=batch_size, verbose=2)\n",
    "    print(\"eval\")\n",
    "    model.evaluate(val_x,  val_y, batch_size=batch_size, verbose=2)\n",
    "\n",
    "    # evaulate\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "147/147 - 2s - loss: 0.1100 - accuracy: 0.9878 - 2s/epoch - 15ms/step\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    test_x=np.array(X_test)\n",
    "    test_y=np.array(Y_test)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"test\")\n",
    "    \n",
    "    model.evaluate(test_x,  test_y, verbose=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33c99ce92ae57a37c90157b7c35934b668ba43f0729fdf7df53b30c6721ba8f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
